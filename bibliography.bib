% Encoding: UTF-8

@Article{parameter_types,
  author    = {Duquia, Rodrigo Pereira and Bastos, Jo{\~a}o Luiz and Bonamigo, Renan Rangel and Gonz{\'a}lez-Chica, David Alejandro and Mart{\'\i}nez-Mesa, Jeovany},
  title     = {{Presenting data in tables and charts}},
  journal   = {Anais brasileiros de dermatologia},
  year      = {2014},
  volume    = {89},
  number    = {2},
  pages     = {280--285},
  publisher = {SciELO Brasil},
}

@Article{Gratch_1996,
  author       = {Gratch, Jonathan and Chien, Steve},
  title        = {{Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study}},
  volume       = {4},
  number       = {1},
  pages        = {365--396},
  abstract     = {Although most scheduling problems are NP-hard, domain specific techniques perform well inpractice  but  are  quite  expensive  to  construct.    In  adaptive  problem-solving,  domain  specificknowledge is acquired automatically for a general problem solver with a flexible control architecture.In this approach, a learning system explores a space of possible heuristic methods for one well-suitedto  the  eccentricities  of  the  given  domain  and  problem  distribution.    In  this  article,  we  discuss  anapplication  of  the  approach  to  scheduling  satellite  communications.    Using  problem  distributionsbased on actual mission requirements, our approach identifies strategies that not only decrease theamount of CPU time required to produce schedules, but also increase the percentage of problems thatare solvable within computational resource limitations},
  acmid        = {1622751},
  year         = {1996},
  issue_date   = {January 1996},
  journal = {Journal of Artificial Intelligence Research},
  location     = {USA},
  numpages     = {32},
  publisher    = {AI Access Foundation},
}

@InProceedings{SMAC_mainpaper,
  author    = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
  title     = {{Sequential Model-Based Optimization for General Algorithm Configuration}},
  booktitle = {Learning and Intelligent Optimization},
  pages     = {507--523},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.},
  year      = {2011},
  location  = {Berlin, Heidelberg},
}

@Article{ParamILS_mainpaper,
  author       = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin and St{\"u}tzle, Thomas},
  title        = {{ParamILS: An Automatic Algorithm Configuration Framework}},
  volume       = {36},
  number       = {1},
  pages        = {267--306},
  abstract     = {The identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm configuration problem. More formally, we provide methods for optimizing a target algorithm's performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for SAT. We also present what is, to our knowledge, the first published work on automatically configuring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identified with considerable effort. Nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements.},
  acmid        = {1734959},
  year         = {2009},
  issue_date   = {September 2009},
  journal = {Journal of Artificial Intelligence Research},
  location     = {USA},
  numpages     = {40},
  publisher    = {AI Access Foundation},
}

@Article{Gratch_1992,
  author={Jonathan Gratch and Gerald DeJong},
  title={{COMPOSER: A Probabilistic Solution to the Utility Problem in Speed-Up Learning}},
  year={1992},
  cdate={694224000000},
  pages={235-240},
  booktitle={AAAI},
  journal = {Proceedings Tenth National Conference on Artificial Intelligence},
  publisher={ERIC},
}

@InProceedings{Minton_1993,
  author    = {Minton, Steven},
  title     = {{An Analytic Learning System for Specializing Heuristics}},
  booktitle = {Proceedings of the 13\textsuperscript{th} International Joint Conference on Artifical Intelligence - Volume 2},
  series    = {IJCAI'93},
  pages     = {922--928},
  abstract  = {This paper describes how meta-level theories are used for analytic learning in MULTI-TAC. MULTI-TAC operationalizes generic heuristics for constraint-satisfaction problems, in order to create programs that are tailored to specific problems. For each of its generic heuristics, MULTI-TAC has a meta-theory specifically designed for operationalising that heuristic. We present examples of the specialisation process and discuss how the theories influence the tractability of the learning process. We also describe an empirical study showing that the specialised programs produced by MULTITAC compare favorably to hand-coded programs.},
  acmid     = {1624153},
  year      = {1993},
  location  = {Chambery, France},
  numpages  = {7},
}

@article{Beamsearch_disadv,
title = {{Stateâ€“Space Search: Algorithms, Complexity, Extensions, and Applications}},
journal = "Computer Standards \& Interfaces",
volume = "22",
number = "3",
pages = "229 - 230",
year = "2000",
author = "Hartmut Moeck"
}

@Article{Coy_2001,
  author       = {Coy, Steven P. and Golden, Bruce L. and Runger, George C. and Wasil, Edward A.},
  title        = {{Using Experimental Design to Find Effective Parameter Settings for Heuristics}},
  volume       = {7},
  number       = {1},
  pages        = {77--97},
  abstract     = {In this paper, we propose a procedure, based on statistical design of experiments and gradient descent, that finds effective settings for parameters found in heuristics. We develop our procedure using four experiments. We use our procedure and a small subset of problems to find parameter settings for two new vehicle routing heuristics. We then set the parameters of each heuristic and solve 19 capacity-constrained and 15 capacity-constrained and route-length-constrained vehicle routing problems ranging in size from 50 to 483 customers. We conclude that our procedure is an effective method that deserves serious consideration by both researchers and operations research practitioners.},
  year         = {2001-01},
  day          = {01},
  journal = {Journal of Heuristics},
}

@Article{Adenso-Diaz_2006,
  author       = {Adenso-Diaz, Belarmino and Laguna, Manuel},
  title        = {{Fine-Tuning of Algorithms Using Fractional Experimental Designs and Local Search}},
  volume       = {54},
  number       = {1},
  pages        = {99--114},
  abstract     = {Researchers and practitioners frequently spend more time fine-tuning algorithms than designing and implementing them. This is particularly true when developing heuristics and metaheuristics, where the right choice of values for search parameters has a considerable effect on the performance of the procedure. When testing metaheuristics, performance typically is measured considering both the quality of the solutions obtained and the time needed to find them. In this paper, we describe the development of CALIBRA, a procedure that attempts to find the best values for up to five search parameters associated with a procedure under study. Because CALIBRA uses Taguchis fractional factorial experimental designs coupled with a local search procedure, the best values found are not guaranteed to be optimal. We test CALIBRA on six existing heuristic-based procedures. These experiments show that CALIBRA is able to find parameter values that either match or improve the performance of the procedures resulting from using the parameter values suggested by their developers. The latest version of CALIBRA can be downloaded for free from the website that appears in the online supplement of this paper at http://or.pubs.informs.org/Pages.collect.html.},
  acmid        = {1246539},
  year         = {2006},
  issue_date   = {January-February 2006},
  journal = {Operations Research},
  keywords     = {Taguchi design of experiments, heuristic search, parameter setting},
  location     = {Institute for Operations Research and the Management Sciences (INFORMS), Linthicum, Maryland, USA},
  numpages     = {16},
  publisher    = {INFORMS},
}


@InProceedings{GA_Terashima_1999,
  author    = {Terashima-Mar\'{\i}n, Hugo and Ross, Peter and Valenzuela-Rend\'{o}n, Manuel},
  title     = {{Evolution of Constraint Satisfaction Strategies in Examination Timetabling}},
  booktitle = {Proceedings of the 1\textsuperscript{st} Annual Conference on Genetic and Evolutionary Computation - Volume 1},
  series    = {GECCO'99},
  pages     = {635--642},
  abstract  = {This paper describes an investigation of solving Examination Timetabling Problems (ETTPs) with Genetic Algorithms (GAs) using a non-direct chromosome representation based on evolving the configuration of Constraint Satisfaction methods. There are two aims. The first is to circumvent the problems posed by a direct chromosome representation for the ETTP that consists of an array of events in which each value represents the timeslot which the corresponding event is assigned to. The second is to show that the adaptation of particular features in both the instance of the problem to be solved and the strategies used to solve it provides encouraging results for real ETTPs. There is much scope for investigating such approaches further, not only for the ETTP, but also for other related scheduling problems.},
  acmid     = {2933984},
  year      = {1999},
  location  = {Orlando, Florida},
  numpages  = {8},
}

@book{Spall_2003,
  title     = {{Introduction to Stochastic Search and Optimization}},
  publisher = {John Wiley \& Sons, Inc.},
  year      = {2003},
  author    = {Spall, James C.},
  edition   = {1},
}

@InProceedings{Bartz_2006,
  author    = {Bartz-Beielstein, Thomas},
  title     = {{Designs for Computer Experiments}},
  booktitle = {Experimental Research in Evolutionary Computation: The New Experimentalism},
  pages     = {79--92},
  publisher = {Springer-Verlag},
  abstract  = {Finding appropriate values for the parameters of an algorithm is a challenging, important, and time consuming task. While typically parameters are tuned by hand, recent studies have shown that automatic tuning procedures can effectively handle this task and often find better parameter settings. F-Race has been proposed specifically for this purpose and it has proven to be very effective in a number of cases. F-Race is a racing algorithm that starts by considering a number of candidate parameter settings and eliminates inferior ones as soon as enough statistical evidence arises against them. In this paper, we propose two modifications to the usual way of applying F-Race that on the one hand, make it suitable for tuning tasks with a very large number of initial candidate parameter settings and, on the other hand, allow a significant reduction of the number of function evaluations without any major loss in solution quality. We evaluate the proposed modifications on a number of stochastic local search algorithms and we show their effectiveness.},
  year      = {2006},
  location  = {Berlin, Heidelberg},
}


@InProceedings{SPOplus_2009,
  author    = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin and Murphy, Kevin P.},
  title     = {{An Experimental Investigation of Model-based Parameter Optimisation: SPO and Beyond}},
  booktitle = {Proceedings of the 11\textsuperscript{th} Annual Conference on Genetic and Evolutionary Computation},
  series    = {GECCO '09},
  pages     = {271--278},
  acmid     = {1569940},
  year      = {2009},
  keywords  = {active learning, gaussian processes, noisy optimization, parameter tuning, sequential experimental design},
  location  = {Montreal, Qu\&\#233;bec, Canada},
  numpages  = {8},
}

@InProceedings{TBSPO_2010,
  author    = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin and Murphy, Kevin},
  title     = {{Time-Bounded Sequential Parameter Optimization}},
  booktitle = {Learning and Intelligent Optimization},
  pages     = {281--298},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The optimization of algorithm performance by automatically identifying good parameter settings is an important problem that has recently attracted much attention in the discrete optimization community. One promising approach constructs predictive performance models and uses them to focus attention on promising regions of a design space. Such methods have become quite sophisticated and have achieved significant successes on other problems, particularly in experimental design applications. However, they have typically been designed to achieve good performance only under a budget expressed as a number of function evaluations (e.g., target algorithm runs). In this work, we show how to extend the Sequential Parameter Optimization framework SPO; see 5] to operate effectively under time bounds. Our methods take into account both the varying amount of time required for different algorithm runs and the complexity of model building and evaluation; they are particularly useful for minimizing target algorithm runtime. Specifically, we avoid the up-front cost of an initial design, introduce a time-bounded intensification mechanism, and show how to reduce the overhead incurred by constructing and using models. Overall, we show that our method represents a new state of the art in model-based optimization of algorithms with continuous parameters on single problem instances.},
  year      = {2010},
  location  = {Berlin, Heidelberg},
}

@InCollection{Racing_firstpaper,
  author    = {Oded Maron and Andrew W. Moore},
  title     = {{Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation}},
  booktitle = {Advances in Neural Information Processing Systems 6},
  pages     = {59--66},
  year      = {1994},
}

@InProceedings{FRace_paper,
  author    = {Birattari, Mauro and St\"{u}tzle, Thomas and Paquete, Luis and Varrentrapp, Klaus},
  title     = {{A Racing Algorithm for Configuring Metaheuristics}},
  booktitle = {Proceedings of the 4\textsuperscript{th} Annual Conference on Genetic and Evolutionary Computation},
  series    = {GECCO'02},
  pages     = {11--18},
  acmid     = {2955494},
  year      = {2002},
  location  = {New York City, New York},
  numpages  = {8},
}

@InProceedings{IFRace_paper,
  author    = {Balaprakash, Prasanna and Birattari, Mauro and St{\"u}tzle, Thomas},
  title     = {{Improvement Strategies for the F-Race Algorithm: Sampling Design and Iterative Refinement}},
  booktitle = {Hybrid Metaheuristics},
  pages     = {108--122},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Finding appropriate values for the parameters of an algorithm is a challenging, important, and time consuming task. While typically parameters are tuned by hand, recent studies have shown that automatic tuning procedures can effectively handle this task and often find better parameter settings. F-Race has been proposed specifically for this purpose and it has proven to be very effective in a number of cases. F-Race is a racing algorithm that starts by considering a number of candidate parameter settings and eliminates inferior ones as soon as enough statistical evidence arises against them. In this paper, we propose two modifications to the usual way of applying F-Race that on the one hand, make it suitable for tuning tasks with a very large number of initial candidate parameter settings and, on the other hand, allow a significant reduction of the number of function evaluations without any major loss in solution quality. We evaluate the proposed modifications on a number of stochastic local search algorithms and we show their effectiveness.},
  year      = {2007},
  location  = {Berlin, Heidelberg},
}

@InProceedings{FRace_limitation2,
  author    = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
  title     = {{Automated Configuration of Mixed Integer Programming Solvers}},
  booktitle = {Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems},
  pages     = {186--202},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {State-of-the-art solvers for mixed integer programming (MIP) problems are highly parameterized, and finding parameter settings that achieve high performance for specific types of MIP instances is challenging. We study the application of an automated algorithm configuration procedure to different MIP solvers, instance types and optimization objectives. We show that this fully-automated process yields substantial improvements to the performance of three MIP solvers: Cplex, Gurobi, and lpsolve. Although our method can be used ``out of the box'' without any domain knowledge specific to MIP, we show that it outperforms the Cplex special-purpose automated tuning tool.},
  year      = {2010},
  location  = {Berlin, Heidelberg},
}

@Article{FRace_limitation3,
  author       = {Ashiqur R. KhudaBukhsh and Lin Xu and Holger H. Hoos and Kevin Leyton-Brown},
  title        = {{SATenstein: Automatically building local search SAT solvers from components}},
  volume       = {232},
  pages        = {20--42},
  abstract     = {Designing high-performance solvers for computationally hard problems is a difficult and often time-consuming task. Although such design problems are traditionally solved by the application of human expertise, we argue instead for the use of automatic methods. In this work, we consider the design of stochastic local search (SLS) solvers for the propositional satisfiability problem (SAT). We first introduce a generalized, highly parameterized solver framework, dubbed SATenstein, that includes components drawn from or inspired by existing high-performance SLS algorithms for SAT. The parameters of SATenstein determine which components are selected and how these components behave; they allow SATenstein to instantiate many high-performance solvers previously proposed in the literature, along with trillions of novel solver strategies. We used an automated algorithm configuration procedure to find instantiations of SATenstein that perform well on several well-known, challenging distributions of SAT instances. Our experiments show that SATenstein solvers achieved dramatic performance improvements as compared to the previous state of the art in SLS algorithms; for many benchmark distributions, our new solvers also significantly outperformed all automatically tuned variants of previous state-of-the-art algorithms.},
  year         = {2016},
  journal = {Artificial Intelligence},
  keywords     = {SAT, Stochastic local search, Automatic algorithm configuration},
}

@InProceedings{FRace_limitation4,
  author    = {Tompkins, Dave A. D. and Hoos, Holger H.},
  title     = {{Dynamic Scoring Functions with Variable Expressions: New SLS Methods for Solving SAT}},
  booktitle = {Theory and Applications of Satisfiability Testing -- SAT 2010},
  pages     = {278--292},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {We introduce a new conceptual model for representing and designing Stochastic Local Search (SLS) algorithms for the propositional satisfiability problem (SAT). Our model can be seen as a generalization of existing variable weighting, scoring and selection schemes; it is based upon the concept of Variable Expressions (VEs), which use properties of variables in dynamic scoring functions. Algorithms in our model are constructed from conceptually separated components: variable filters, scoring functions (VEs), variable selection mechanisms and algorithm controllers. To explore the potential of our model we introduce the Design Architecture for Variable Expressions (DAVE), a software framework that allows users to specify arbitrarily complex algorithms at run-time. Using DAVE, we can easily specify rich design spaces of SLS algorithms and subsequently explore these using an automated algorithm configuration tool. We demonstrate that by following this approach, we can achieve significant improvements over previous state-of-the-art SLS-based SAT solvers on software verification benchmark instances from the literature.},
  year      = {2010},
  location  = {Berlin, Heidelberg},
}

@Article{Bayesianoptimization_review,
  author       = {B. {Shahriari} and K. {Swersky} and Z. {Wang} and R. P. {Adams} and N. {de Freitas}},
  title        = {{Taking the Human Out of the Loop: A Review of Bayesian Optimization}},
  volume       = {104},
  number       = {1},
  pages        = {148--175},
  abstract     = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  year         = {2016},
  journal = {Proceedings of the IEEE},
  keywords     = {Bayes methods;Big Data;optimisation;storage al123456;Bayesian optimization;human productivity;product quality;storage architecture;large-scale heterogeneous computing;massive complex software system;Big data application;Big data;Bayes methods;Linear programming;Decision making;Design of experiments;Optimization;Genomes;Statistical analysis;decision making;design of experiments;optimization;response surface methodology;statistical learning;genomic medicine;Decision making;design of experiments;optimization;response surface methodology;statistical learning},
}

@Article{AACfirstwork,
  author       = {Steven Minton and Mark D. Johnston and Andrew B. Philips and Philip Laird},
  title        = {{Minimizing Conflicts: A Heuristic Repair Method for Constraint Satisfaction and Scheduling Problems}},
  volume       = {58},
  pages        = {161--205},
  abstract     = {The paper describes a simple heuristic approach to solving large-scale constraint satisfaction and scheduling problems. In this approach one starts with an inconsistent assignment for a set of variables and searches through the space of possible repairs. The search can be guided by a value-ordering heuristic, the min-conflicts heuristic , that attempts to minimize the number of constraint violations after each step. The heuristic can be used with a variety of different search strategies. We demonstrate empirically that on the n -queens problem, a technique based on this approach performs orders of magnitude better than traditional backtracking techniques. We also describe a scheduling application where the approach has been used successfully. A theoretical analysis is presented both to explain why this method works well on certain types of problems and to predict when it is likely to be most effective.},
  year         = {1992},
  journal = {Artificial Intelligence},
}

@Article{Neldermead_1965,
  author       = {Nelder, John A and Mead, Roger},
  title        = {{A Simplex Method for Function Minimization}},
  volume       = {7},
  number       = {4},
  pages        = {308--313},
  abstract     = {{A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.}},
  year         = {1965},
  journal = {The Computer Journal},
}

@Article{Burmen_2006,
  author       = {B{\H{u}}rmen, {\'A}rp{\'a}d and Puhan, Janez and Tuma, Tadej},
  title        = {{Grid Restrained Nelder-Mead Algorithm}},
  volume       = {34},
  number       = {3},
  pages        = {359--375},
  abstract     = {Probably the most popular algorithm for unconstrained minimization for problems of moderate dimension is the Nelder-Mead algorithm published in 1965. Despite its age only limited convergence results exist. Several counterexamples can be found in the literature for which the algorithm performs badly or even fails. A convergent variant derived from the original Nelder-Mead algorithm is presented. The proposed algorithm's convergence is based on the principle of grid restrainment and therefore does not require sufficient descent as the recent convergent variant proposed by Price, Coope, and Byatt. Convergence properties of the proposed grid-restrained algorithm are analysed. Results of numerical testing are also included and compared to the results of the algorithm proposed by Price et al. The results clearly demonstrate that the proposed grid-restrained algorithm is an efficient direct search method.},
  year         = {2006},
  day          = {01},
  journal = {Computational Optimization and Applications},
}

@InProceedings{Modelbased_GGA,
  author    = {Ans\'{o}tegui, Carlos and Malitsky, Yuri and Samulowitz, Horst and Sellmann, Meinolf and Tierney, Kevin},
  title     = {{Model-based Genetic Algorithms for Algorithm Configuration}},
  booktitle = {Proceedings of the 24\textsuperscript{th} International Conference on Artificial Intelligence},
  series    = {IJCAI'15},
  pages     = {733--739},
  publisher = {AAAI Press},
  abstract  = {Automatic algorithm configurators are important practical tools for improving program performance measures, such as solution time or prediction accuracy. Local search approaches in particular have proven very effective for tuning algorithms. In sequential local search, the use of predictive models has proven beneficial for obtaining good tuning results. We study the use of non-parametric models in the context of population-based algorithm configurators. We introduce a new model designed specifically for the task of predicting high-performance regions in the parameter space. Moreover, we introduce the ideas of genetic engineering of offspring as well as sexual selection of parents. Numerical results show that model-based genetic algorithms significantly improve our ability to effectively configure algorithms automatically.},
  acmid     = {2832351},
  year      = {2015},
  location  = {Buenos Aires, Argentina},
  numpages  = {7},
}

@InProceedings{GGA_application1,
  author    = {Kadioglu, Serdar and Malitsky, Yuri and Sellmann, Meinolf and Tierney, Kevin},
  title     = {{ISAC --Instance-Specific Algorithm Configuration}},
  booktitle = {Proceedings of the 2010 Conference on ECAI 2010: 19\textsuperscript{th} European Conference on Artificial Intelligence},
  pages     = {751--756},
  publisher = {IOS Press},
  abstract  = {We present a new method for instance-specific algorithm configuration (ISAC). It is based on the integration of the algorithm configuration system GGA and the recently proposed stochastic offline programming paradigm. ISAC is provided a solver with categorical, ordinal, and/or continuous parameters, a training benchmark set of input instances for that solver, and an algorithm that computes a feature vector that characterizes any given instance. ISAC then provides high quality parameter settings for any new input instance. Experiments on a variety of different constrained optimization and constraint satisfaction solvers show that automatic algorithm configuration vastly outperforms manual tuning. Moreover, we show that instance-specific tuning frequently leads to significant speed-ups over instance-oblivious configurations.},
  acmid     = {1861114},
  year      = {2010},
  location  = {Amsterdam, The Netherlands, The Netherlands},
  numpages  = {6},
}

@InProceedings{GGA_application2,
  author    = {Malitsky, Yuri and Sabharwal, Ashish and Samulowitz, Horst and Sellmann, Meinolf},
  title     = {{Algorithm Portfolios Based on Cost-sensitive Hierarchical Clustering}},
  booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
  series    = {IJCAI '13},
  pages     = {608--614},
  publisher = {AAAI Press},
  abstract  = {Different solution approaches for combinatorial problems often exhibit incomparable performance that depends on the concrete problem instance to be solved. Algorithm portfolios aim to combine the strengths of multiple algorithmic approaches by training a classifier that selects or schedules solvers dependent on the given instance. We devise a new classifier that selects solvers based on a cost-sensitive hierarchical clustering model. Experimental results on SAT and MaxSAT show that the new method outperforms the most effective portfolio builders to date.},
  acmid     = {2540217},
  year      = {2013},
  location  = {Beijing, China},
  numpages  = {7},
}

@InProceedings{SMAC_extendedpaper,
  author      = {Hutter F, Hoos H, Leyton-Brown K},
  title       = {{Sequential model-based optimization for general algorithm configuration (extended version)}},
  publisher = {University of British Columbia},
  booktitle        = {{Technical Report. TR-2010-10}},
  year        = {2010},
  location    = {University of British Columbia, Department of Computer Science.},
}

@InProceedings{GGA_paper,
  author    = {Ans{\'o}tegui, Carlos and Sellmann, Meinolf and Tierney, Kevin},
  title     = {{A Gender-Based Genetic Algorithm for the Automatic Configuration of Algorithms}},
  booktitle = {{Principles and Practice of Constraint Programming}},
  pages     = {142--157},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {A problem that is inherent to the development and efficient use of solvers is that of tuning parameters. The CP community has a long history of addressing this task automatically. We propose a robust, inherently parallel genetic algorithm for the problem of configuring solvers automatically. In order to cope with the high costs of evaluating the fitness of individuals, we introduce a gender separation whereby we apply different selection pressure on both genders. Experimental results on a selection of SAT solvers show significant performance and robustness gains over the current state-of-the-art in automatic algorithm configuration.},
  year      = {2009},
  location  = {Berlin, Heidelberg},
}

@article{SMAC_ParamILS_GGA_compare,
  title={{The configurable SAT solver challenge (CSSC)}},
  author={Hutter, Frank and Lindauer, Marius and Balint, Adrian and Bayless, Sam and Hoos, Holger and Leyton-Brown, Kevin},
  journal={Artificial Intelligence},
  volume={243},
  pages={1--25},
  year={2017},
  publisher={Elsevier},
}

@article{Pitfalls,
  author       = {Eggensperger, Katharina and Lindauer, Marius and Hutter, Frank},
  title        = {{Pitfalls and Best Practices in Algorithm Configuration}},
  volume       = {64},
  number       = {1},
  pages        = {861--893},
  abstract     = {Good parameter settings are crucial to achieve high performance in many areas of artificial intelligence (AI), such as propositional satisfiability solving, AI planning, scheduling, and machine learning (in particular deep learning). Automated algorithm configuration methods have recently received much attention in the AI community since they replace tedious, irreproducible and error-prone manual parameter tuning and can lead to new state-of-the-art performance. However, practical applications of algorithm configuration are prone to several (often subtle) pitfalls in the experimental design that can render the procedure ineffective. We identify several common issues and propose best practices for avoiding them. As one possibility for automatically handling as many of these as possible, we also propose a tool called GenericWrapper4AC.},
  acmid        = {3336187},
  year         = {2019},
  issue_date   = {January 2019},
  journal = {Journal of Artificial Intelligence Research},
  location     = {USA},
  numpages     = {33},
  publisher    = {AI Access Foundation},
}

@InProceedings{Expertdown1,
  author    = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
  title     = {{Practical Bayesian Optimization of Machine Learning Algorithms}},
  booktitle = {Proceedings of the 25\textsuperscript{th} International Conference on Neural Information Processing Systems - Volume 2},
  series    = {NIPS'12},
  pages     = {2951--2959},
  address   = {USA},
  acmid     = {2999464},
  year      = {2012},
  location  = {Lake Tahoe, Nevada},
  numpages  = {9},
}

@InProceedings{Expertdown2,
  author    = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
  title     = {{Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms}},
  booktitle = {Proceedings of the 19\textsuperscript{th} ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  series    = {KDD '13},
  pages     = {847--855},
  acmid     = {2487629},
  year      = {2013},
  keywords  = {hyperparameter optimization, model selection, weka},
  location  = {Chicago, Illinois, USA},
  numpages  = {9},
}

@InProceedings{Expertdown3,
  author    = {Bergstra, J. and Yamins, D. and Cox, D. D.},
  title     = {{Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures}},
  booktitle = {Proceedings of the 30\textsuperscript{th} International Conference on International Conference on Machine Learning - Volume 28},
  series    = {ICML'13},
  pages     = {I-115--I-123},
  publisher = {JMLR.org},
  acmid     = {3042832},
  year      = {2013},
  location  = {Atlanta, GA, USA},
}

@Article{Expertdown4,
  author       = {Lang, M. and Kotthaus, Helena and Marwedel, Peter and Weihs, Claus and Rahnenf{\"{u}}hrer, J{\"{o}}rg and Bischl, Bernd},
  title        = {{Automatic model selection for high-dimensional survival analysis}},
  volume       = {85},
  year         = {2015},
  journal = {Journal of Statistical Computation and Simulation},
}

@Article{AC_benchmarking,
  author       = {Eggensperger, Katharina and Lindauer, Marius and Hoos, Holger H. and Hutter, Frank and Leyton-Brown, Kevin},
  title        = {{Efficient benchmarking of algorithm configurators via model-based surrogates}},
  volume       = {107},
  number       = {1},
  pages        = {15--41},
  abstract     = {The optimization of algorithm (hyper-)parameters is crucial for achieving peak performance across a wide range of domains, ranging from deep neural networks to solvers for hard combinatorial problems. However, the proper evaluation of new algorithm configuration (AC) procedures (or configurators) is hindered by two key hurdles. First, AC scenarios are hard to set up, including the target algorithm to be optimized and the problem instances to be solved. Second, and even more significantly, they are computationally expensive: a single configurator run involves many costly runs of the target algorithm. Here, we propose a benchmarking approach that uses surrogate scenarios, which are computationally cheap while remaining close to the original AC scenarios. These surrogate scenarios approximate the response surface corresponding to true target algorithm performance using a regression model. In our experiments, we construct and evaluate surrogate scenarios for hyperparameter optimization as well as for AC problems that involve performance optimization of solvers for hard combinatorial problems. We generalize previous work by building surrogates for AC scenarios with multiple problem instances, stochastic target algorithms and censored running time observations. We show that our surrogate scenarios capture overall important characteristics of the original AC scenarios from which they were derived, while being much easier to use and orders of magnitude cheaper to evaluate.},
  year         = {2018},
  day          = {01},
  journal = {Machine Learning},
}

@InProceedings{Deterministic_optimization,
  pages     = {77--102},
  title     = {{Deterministic Optimization}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Cavazzuti, Marco},
  abstract  = {Among the possible classifications of the optimization algorithms we decided to divide them into two categories: deterministic and stochastic. By deterministic optimization all the algorithms that follow a rigorous mathematical approach are intended. Strictly speaking this refers to mathematical programming. After introducing the terminology used in this field, line-search and trust region strategies are described. The chapter is further subdivided into two major sections: unconstrained and constrained optimization. In the first several optimization methods are introduced such as simplex, Newton and quasi-Newton, conjugate directions, and Levenberg--Marquardt. In the latter, due to the complexity of the topic only a brief discussion on the main aspects and approaches found in constrained optimization algorithms is given. These are elimination methods, Lagrangian methods, active set methods, 0s methods. Sequential quadratic programming and mixed integer programming are also introduced. In the conclusions, the different algorithms are discussed in terms of their simplicity, reliability, and efficiency.},
  booktitle = {Optimization Methods: From Theory to Design Scientific and Technological Aspects in Mechanics},
  year      = {2013},
  location  = {Berlin, Heidelberg},
}

@Article{Deterministic_optimization_Review,
  author       = {Lin, Ming-Hua and Tsai, Jung-Fa and Yu, Chian-Son},
  title        = {{A Review of Deterministic Optimization Methods in Engineering and Management}},
  year         = {2012},
  journal = {Mathematical Problems in Engineering},
}

@Incollection{AAC_Mainreview,
  pages     = {37--71},
  title     = {{Automated Algorithm Configuration and Parameter Tuning}},
  publisher = {Springer Berlin Heidelberg},
  author    = {Hoos, Holger H.},
  abstract  = {The use of automated configuration and parameter tuning techniques plays an increasingly important role in the design, evaluation and application of high-performance algorithms for difficult computational problems. This chapter provides an introduction to these techniques and gives an overview of three families of procedures for automatically optimising the performance of parameterised algorithms. Racing procedures iteratively evaluate parameter settings on problem instances from a given set and use statistical hypothesis tests to eliminate candidate configurations that are significantly outperformed by other configurations. ParamILS uses a powerful stochastic local search method to search within potentially vast spaces of candidate configurations of a given algorithm. And finally, sequential model-based optimisation (SMBO) methods build a response surface model that relates parameter settings to performance, and use this model to iteratively identify promising settings. We also briefly survey other algorithm configuration and parameter tuning procedures, as well as related approaches, such as instance-based algorithm selection and configuration.},
  booktitle = {Autonomous Search},
  year      = {2012},
  location  = {Berlin, Heidelberg},
}

@InProceedings{OscarLima_SMAC,
  author    = {O. {Lima} and R. {Ventura}},
  title     = {{A case study on automatic parameter optimization of a mobile robot localization algorithm}},
  booktitle = {2017 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
  pages     = {43--48},
  year      = {2017},
  keywords  = {Boolean algebra;computability;mobile robots;Monte Carlo methods;operating systems (computers);optimisation;automatic parameter optimization;mobile robot localization algorithm;Boolean satisfiability problem solvers;sequential model-based optimization;automatic parameter tuning;adaptive Monte Carlo localization algorithm;open source experimental setup;robot operating system;ROS;Robot sensing systems;Machine learning algorithms;Optimization;Servers;Model based optimization robotics;black box optimization;algorithm configuration;automatic algorithm optimization;multi objective parameter optimization;automatic parameter tuning},
}

@Misc{MpCCI_documentation,
  title     = {{MpCCI 4.5.2-1 Documentation}},
  author    = {Fraunhofer Institute for Algorithms and Scientific Computing SCAI},
  edition   = {Part 1 Overview},
  year      = {2018},
  abstract  = {"MpCCI Coupling-Environment" is the standard for simulation code coupling.
In this manual MpCCI will be used as abbreviation for "MpCCI Coupling-Environment".
MpCCI has been developed at the Fraunhofer Institute SCAI in order to provide an application independent
interface for the coupling of different simulation codes.},
  date      = {2018-05},
  location  = {Fraunhofer Institute for Algorithms and Scientific Computing SCAI Schloss Birlinghoven, 53754 Sankt Augustin, Germany},
  timestamp = {04-05-2019},
  url       = {https://www.mpcci.de/content/dam/scai/mpcci/documents/MpCCIdoc-452-20180504.pdf},
  note         = {Accessed on: 2019-05-22. [Online].},
}

@InProceedings{Bharath_SMAC,
  author    = {R. {Burger} and M. {Bharatheesha} and M. {van Eert} and R. {Babu{\v{s}}ka}},
  title     = {{Automated tuning and configuration of path planning algorithms}},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages     = {4371--4376},
  abstract  = {A large number of novel path planning methods for a wide range of problems have been described in literature over the past few decades. These algorithms can often be configured using a set of parameters that greatly influence their performance. In a typical use case, these parameters are only very slightly tuned or even left untouched. Systematic approaches to tune parameters of path planning algorithms have been largely unexplored. At the same time, there is a rising interest in the planning and robotics communities regarding the real world application of these theoretically developed and simulation-tested planning algorithms. In this work, we propose the use of Sequential Model-based Algorithm Configuration (SMAC) tools to address these concerns. We show that it is possible to improve the performance of a planning algorithm for a specific problem without the need of in-depth knowledge of the algorithm itself. We compare five planners that see a lot of practical usage on three typical industrial pick-and-place tasks to demonstrate the effectiveness of the method.},
  year      = {2017},
  keywords  = {control system synthesis;industrial manipulators;path planning;path planning algorithms;automated tuning;robotics communities;sequential model-based algorithm configuration tools;SMAC;industrial pick-and-place tasks;Planning;Tuning;Robots;Algorithm design and analysis;Software algorithms;Prediction algorithms;Software},
}

@article{criticalinstances,
  title={{Added-mass effect in the design of partitioned algorithms for fluid--structure problems}},
  author={Causin, Paola and Gerbeau, Jean-Fr{\'e}d{\'e}ric and Nobile, Fabio},
  journal={Computer methods in applied mechanics and engineering},
  volume={194},
  number={42-44},
  pages={4506--4527},
  year={2005},
  publisher={Elsevier}
}

@article{outlier,
  title={{Outliers in Statistical Data}},
  author={Barnett, V., and Lewis T.},
  journal={Biometrical Journal},
  volume={37},
  number={2},
  year={1995},
  publisher={Wiley Online Library}
}


@PhdThesis{couplingscheme,
  author = {Sieber, Galina},
  title  = {{Numerical simulation of fluid-structure interaction using loose coupling methods}},
  school = {Technische Universit{\"a}t Darmstadt},
  year   = {2002},
}

@Article{FSI_Bungartz,
  author       = {Hans-Joachim Bungartz and Florian Lindner and Bernhard Gatzhammer and Miriam Mehl and Klaudius Scheufele and Alexander Shukaev and Benjamin Uekermann},
  title        = {{preCICE -- A fully parallel library for multi-physics surface coupling}},
  volume       = {141},
  pages        = {250--258},
  abstract     = {In the emerging field of multi-physics simulations, we often face the challenge to establish new connections between physical fields, to add additional aspects to existing models, or to exchange a solver for one of the involved physical fields. If in such cases a fast prototyping of a coupled simulation environment is required, a partitioned setup using existing codes for each physical field is the optimal choice. As accurate models require also accurate numerics, multi-physics simulations typically use very high grid resolutions and, accordingly, are run on massively parallel computers. Here, we face the challenge to combine flexibility with parallel scalability and hardware efficiency. In this paper, we present the coupling tool preCICE which offers the complete coupling functionality required for a fast development of a multi-physics environment using existing, possibly black-box solvers. We hereby restrict ourselves to bidirectional surface coupling which is too expensive to be done via file communication, but in contrast to volume coupling still a candidate for distributed memory parallelism between the involved solvers. The paper gives an overview of the numerical functionalities implemented in preCICE as well as the user interfaces, i.e., the application programming interface and configuration options. Our numerical examples and the list of different open-source and commercial codes that have already been used with preCICE in coupled simulations show the high flexibility, the correctness, and the high performance and parallel scalability of coupled simulations with preCICE as the coupling unit.},
  year         = {2016},
  journal = {Computers \& Fluids},
  keywords     = {Partitioned multi-physics, Strong coupling, Non-matching grids, Inter-code communication, Quasi-Newton, Radial basis functions, High performance computing},
  publisher={Elsevier},
}

@Article{Couplingenv_Gatzhammer,
  author       = {Bernhard Gatzhammer and Miriam Mehl and Tobias Neckel},
  title        = {{A coupling environment for partitioned multiphysics simulations applied to fluid-structure interaction scenarios}},
  volume       = {1},
  number       = {1},
  pages        = {681--689},
  note         = {ICCS 2010},
  abstract     = {An efficient way to simulate multi-physics scenarios is given by the partitioned coupling approach. It allows to take up established simulation codes for single fields and combine them to one multi-physics simulation tool. The challenges appearing there range from purely technical, such as data communication, via numerical, such as data mapping and transient coupling algorithms, to software engineering challenges, in order to pertain the inherent modularity of the partitioned approach. We present the coupling environment preCICE, which provides black box solutions for surface coupling to the tasks mentioned before and serves as basis for the development of new coupling features. As application example we show fluid-structure interaction scenarios simulated with fixed-grid fluid solvers.},
  year         = {2010},
  journal = {Procedia Computer Science},
  keywords     = {Partitioned coupling, Coupling algorithms, Fluid-structure interaction, Coupling tool, Octree},
}

@MastersThesis{FSICartesiangrid,
  author      = {Bernhard Gatzhammer},
  title       = {{A partitioned approach for Fluid-Structure Interaction on cartesian grids.}},
  abstract    = {This work is about the computer-based simulation of problems from fluid-structure interaction (FSI). Fluid-structure interactions are a multiphysics problem and combine fluids, structures, and their bi-directional interactions. Instead of solving one specific problem or applying one specialized approach to the field of FSI, the thesis rather strives
to provide a software environment supporting any kind of investigations into and solutions of partitioned FSI problems. It does so by providing a coupling environment for
two independent simulation programs, one specialized on the simulation of fluids and
the other one on structures. The special idea behind this tool is to encapsulate all
coupling functionality and tools into a unit called coupling supervisor, which minimizes
the amount of work necessary to prepare a simulation program for coupled simulations.
This is achieved with a client-server based software architecture, hiding the coupled simulation programs from each other by a redirection of all communication to the coupling
supervisor and the use of a dedicated coupling mesh for the communication of coupling
data. In order to approve the maturity of the coupling tool, it must be validated by
the simulation of a well defined test scenario defining quantities for comparison with
approved results. This leads to the second part of this work, the preparation of a fluid
simulation program for the simulation of an FSI benchmarking scenario. To qualify a
fluid solver for partitioned FSI simulations, it must be able to handle dynamic obstacles
and to transfer data from its discretization mesh to that of the coupling supervisor.
Finally, numerical results are presented, showing the influence of implemented features
and giving some first ideas about the expected results of the FSI benchmark scenarios.},
  year        = {2008},
  school = {Technische UniversitÃ¼t MÃ¼nchen},
  location    = {Technische UniversitÃ¼t MÃ¼nchen, Munich, Germany.},
}

@Misc{Blood-vessel,
  author = {COMSOL Multiphysics},
  title  = {{COMSOL Multiphysics Tutorial - Fluid-Structure Interaction in a Network of Blood Vessels}},
  note   = {Accessed on: 2019-11-05. [Online].},
  year   = {2019},
  owner  = {COMSOL Multiphysics},
  url    = {https://www.comsol.com/model/fluid-structure-interaction-in-a-network-of-blood-vessels-660},
}

@Misc{blood,
  author = {Dr Zara Kassam},
  title  = {{CRISPR used to improve red blood cell transfusion compatibility}},
  note   = {Accessed on: 2019-11-05. [Online].},
  year   = {2018},
  owner  = {Drug Target Review},
  url    = {https://www.drugtargetreview.com/news/31427/crispr-blood-transfusion-compatibility/},
}

@Misc{vessel,
  author = {Thomas Philip Cahill},
  title  = {{Rough method of creating blood vessel}},
  note   = {Accessed on: 2019-11-05. [Online].},
  year   = {2013},
  owner  = {Grab CAD Community},
  url    = {https://grabcad.com/library/blood-vessel-1},
}

@Misc{MpCCI_documentation_partners,
  author = {Fraunhofer Institute for Algorithms and Scientific Computing},
  title  = {{MpCCI - Research Community}},
  note   = {Accessed on: 2019-11-05. [Online].},
  owner  = {MpCCI},
  year = {2018},
  url    = {https://www.mpcci.de/en/research-community.html#tabpanel-3},
}

@Misc{car-acoustics,
  author = {COMSOL Multiphysics},
  title  = {{COMSOL Multiphysics Tutorial -Car Cabin Acoustics - Frequency Domain Analysis}},
  note   = {Accessed on: 2019-11-05. [Online].},
  owner  = {COMSOL Multiphysics},
  url    = {https://www.comsol.com/model/car-cabin-acoustics-frequency-domain-analysis-15013},
}

@Misc{submarine,
  author = {COMSOL Multiphysics},
  title  = {COMSOL Multiphysics Tutorial -Magnetic Signature of a Submarine},
  note   = {Accessed on: 2019-11-05. [Online].},
  owner  = {COMSOL Multiphysics},
  url    = {https://www.comsol.com/model/magnetic-signature-of-a-submarine-291},
}

@InProceedings{HPO_AC,
  author    = {Tobias Domhan and Jost Tobias Springenberg and Frank Hutter},
  title     = {{Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves}},
  booktitle = {IJCAI},
  abstract  = {Deep neural networks (DNNs) show very strong performance on many machine learning problems, but they are very sensitive to the setting of their hyperparameters. Automated hyperparameter optimization methods have recently been shown to yield settings competitive with those found by human experts, but their widespread adoption is hampered by the fact that they require more computational resources than human experts. Humans have one advantage: when they evaluate a poor hyperparameter setting they can quickly detect (after a few steps of stochastic gradient descent) that the resulting network performs poorly and terminate the corresponding evaluation to save time. In this paper, we mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the first part of a learning curve. Experiments with a broad range of neural network architectures on various prominent object recognition benchmarks show that our resulting approach speeds up state-of-the-art hyperparameter optimization methods for DNNs roughly twofold, enabling them to find DNN settings that yield better performance than those chosen by human experts.},
  year      = {2015},
}

@InCollection{Robust_AutoML,
  author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  title     = {{Efficient and Robust Automated Machine Learning}},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  pages     = {2962--2970},
  abstract  = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn.},
  year      = {2015},
}

@InProceedings{SAT_examplepaper,
  author   = {Hutter, Frank and Babic, Domagoj and Hoos, Holger and Hu, Alan},
  title    = {{Boosting Verification by Automatic Tuning of Decision Procedures}},
  booktitle={Formal Methods in Computer Aided Design, FMCAD},
  pages    = {27--34},
  abstract = {Parameterized heuristics abound in computer aided design and verification, and manual tuning of the respective parameters is difficult and time-consuming. Very recent results from the artificial intelligence (AI) community suggest that this tuning process can be automated, and that doing so can lead to significant performance improvements; furthermore, automated parameter optimization can provide valuable guidance during the development of heuristic algorithms. In this paper, we study how such an AI approach can improve a state-of-the-art SAT solver for large, real-world bounded model-checking and software verification instances. The resulting, automatically-derived parameter settings yielded runtimes on average 4.5 times faster on bounded model checking instances and 500 times faster on software verification problems than extensive hand-tuning of the decision procedure. Furthermore, the availability of automatic tuning influenced the design of the solver, and the automatically-derived parameter settings provided a deeper insight into the properties of problem instances.},
  year     = {2007},
}

@Article{AIplanning,
  author       = {Vallati, Mauro and Fawcett, Chris and Gerevini, Alfonso and Hoos, Holger and Saetti, Alessandro},
  title        = {{Automatic Generation of Efficient Domain-Optimized Planners from Generic Parametrized Planners}},
  abstract     = {When designing state-of-the-art, domain-independent planning sys-tems, many decisions have to be made with respect to the domain analysis or compilation performed during preprocessing, the heuristic functions used during search, and other features of the search algorithm. These design decisions can have a large impact on the performance of the resulting planner. By providing many alternatives for these choices and exposing them as parameters, planning systems can in principle be configured to work well on different domains. How-ever, usually planners are used in default configurations that have been chosen be-cause of their good average performance over a set of benchmark domains, with limited experimentation of the potentially huge range of possible configurations. In this work, we propose a general framework for automatically configuring a pa-rameterized planner, showing that substantial performance gains can be achieved. We apply the framework to the well-known LPG planner, which has 62 parame-ters and over 6.5 Ã— 10 17 possible configurations. We demonstrate that by using this highly parameterized planning system in combination with the off-the-shelf, state-of-the-art automatic algorithm configuration procedure ParamILS, the plan-ner can be specialized obtaining significantly improved performance.},
  year         = {2013},
  journal = {Sixth Annual Symposium on Combinatorial Search, SoCS},
}

@InProceedings{ASP_solver,
  author = {Gebser, Martin and Kaminski, Roland and Kaufmann, Benjamin and Schaub, Torsten and Schneider, Marius and Ziller, Stefan},
  title  = {{A Portfolio Solver for Answer Set Programming: Preliminary Report}},
  booktitle={International Conference on Logic Programming and Nonmonotonic Reasoning},
  pages  = {352--357},
  year   = {2011},
  organization={Springer},
}

@Article{EGO_Basepaper,
  author       = {Jones, Donald R. and Schonlau, Matthias and Welch, William J.},
  title        = {{Efficient Global Optimization of Expensive Black-Box Functions}},
  volume       = {13},
  number       = {4},
  pages        = {455--492},
  abstract     = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
  year         = {1998},
  day          = {01},
  journal = {Journal of Global Optimization},
}

@Article{EPM_hardnessmodels,
  author       = {Leyton-Brown, Kevin and Nudelman, Eugene and Shoham, Yoav},
  title        = {{Empirical Hardness Models: Methodology and a Case Study on Combinatorial Auctions}},
  volume       = {56},
  number       = {4},
  pages        = {22:1--22:52},
  acmid        = {1538906},
  articleno    = {22},
  year         = {2009},
  issue_date   = {June 2009},
  journal = {J. ACM},
  keywords     = {Empirical analysis of algorithms, algorithm portfolios, combinatorial auctions, runtime prediction},
  location     = {New York, NY, USA},
  numpages     = {52},
  publisher    = {ACM},
}

@Article{BayesianOptimization_convergence,
  author       = {Mockus, Jonas},
  title        = {{Application of Bayesian approach to numerical methods of global and stochastic optimization}},
  volume       = {4},
  number       = {4},
  pages        = {347--365},
  abstract     = {In this paper a review of application of Bayesian approach to global and stochastic optimization of continuous multimodal functions is given. Advantages and disadvantages of Bayesian approach (average case analysis), comparing it with more usual minimax approach (worst case analysis) are discussed. New interactive version of software for global optimization is discussed. Practical multidimensional problems of global optimization are considered},
  year         = {1994},
  day          = {01},
  journal = {Journal of Global Optimization},
}

@Article{BayesianOptimization_papertutorials,
  author       = {{Frazier}, Peter I.},
  title        = {{A Tutorial on Bayesian Optimization}},
  pages        = {arXiv:1807.02811},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  year         = {2018},
  eid          = {arXiv:1807.02811},
  eprint       = {1807.02811},
  eprintclass  = {stat.ML},
  eprinttype   = {arXiv},
  journal = {arXiv e-prints},
  keywords     = {Statistics - Machine Learning, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@InProceedings{robotgait,
  author    = {Lizotte, Daniel and Wang, Tao and Bowling, Michael and Schuurmans, Dale},
  title     = {{Automatic Gait Optimization with Gaussian Process Regression}},
  booktitle = {Proceedings of the 20\textsuperscript{th} International Joint Conference on Artifical Intelligence},
  series    = {IJCAI'07},
  pages     = {944--949},
  acmid     = {1625428},
  year      = {2007},
  location  = {Hyderabad, India},
  numpages  = {6},
}

@InProceedings{hponn,
  author    = {Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal\'{a}zs},
  title     = {{Algorithms for Hyper-parameter Optimization}},
  booktitle = {Proceedings of the 24\textsuperscript{th} International Conference on Neural Information Processing Systems},
  year      = {2011},
  series    = {NIPS'11},
  pages     = {2546--2554},
  address   = {USA},
  acmid     = {2986743},
  location  = {Granada, Spain},
  numpages  = {9},
}

@MastersThesis{driven_cavity,
  author      = {Koch, M. D},
  title       = {{Quasi-Newton Methods for Unstable Partitioned Fluid-Structure Interactions}},
  year        = {2016},
  school = {Institut fÃ¼r Numerische Simulation, Rheinische Friedrich-Wilhelms-UniversitÃ¤t Bonn.},
  lauthor     = {Koch, M. D},
  location    = {Bonn, Germany},
}

@inproceedings{robustqn2,
  title={Coupling techniques for partitioned fluid-structure interaction simulations with black-box solvers},
  author={Degroote, Joris and Haelterman, Robby and Annerel, Sebastiaan and Vierendeels, Jan},
  booktitle={10th MpCCI User Forum},
  pages={82--91},
  year={2009},
  organization={Fraunhofer Institute SCAI}
}


@phdthesis{robustqn,
author = {Scheufele, Klaudius},
year = {2015},
title = {Robust Quasi-Newton Methods for Partitioned Fluid-Structure Simulations},
school = {Institute of Parallel and Distributed Systems, University of Stuttgart},
}


@Incollection{2dbenchmark,
author="Kuhlmann, Hendrik C.
and Roman{\`o}, Francesco",
title="The Lid-Driven Cavity",
bookTitle="Computational Modelling of Bifurcations and Instabilities in Fluid Dynamics",
journal = {Computational Methods in Applied Sciences},
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="233--309",
abstract="The lid-driven cavity is an important fluid mechanical system serving as a benchmark for testing numerical methods and for studying fundamental aspects of incompressible flows in confined volumes which are driven by the tangential motion of a bounding wall. A comprehensive review is provided of lid-driven cavity flows focusing on the evolution of the flow as the Reynolds number is increased. Understanding the flow physics requires to consider pure two-dimensional flows, flows which are periodic in one space direction as well as the full three-dimensional flow. The topics treated range from the characteristic singularities resulting from the discontinuous boundary conditions over flow instabilities and their numerical treatment to the transition to chaos in a fully confined cubical cavity. In addition, the streamline topology of two-dimensional time-dependent and of steady three-dimensional flows are covered, as well as turbulent flow in a square and in a fully confined lid-driven cube. Finally, an overview on various extensions of the lid-driven cavity is given.",
}

@Book{relaxationmethod_intro,
  title     = {{Iterative Solution of Nonlinear Equations in Several Variables}},
  publisher = {Society for Industrial and Applied Mathematics},
  author    = {Ortega, James M. and Rheinboldt, Werner C.},
  year      = {2000},
  location  = {Philadelphia, PA, USA},
}


@Article{repeatability1,
  author       = {Colm McAlinden and Jyoti Khadka and Konrad Pesudovs},
  title        = {{Precision (repeatability and reproducibility) studies and sample-size calculation}},
  volume       = {41},
  number       = {12},
  pages        = {2598--2604},
  year         = {2015},
  journal = {Journal of Cataract \& Refractive Surgery},
}

@Article{statistic_definition,
  author       = {Sykes, L M AND Gani, F AND Vally, Z},
  title        = {{Statistical terms Part 1: The meaning of the MEAN, and other statistical terms commonly used in medical research}},
  volume       = {71},
  pages        = {274--278},
  year         = {2016},
  journal = {{South African Dental Journal }},
  language     = {en},
  publisher    = {scieloza},
}

@Article{anova,
  author       = {Sawyer, Steven},
  title        ={{Analysis of Variance: The Fundamental Concepts}},
  volume       = {17},
  pages        = {27E-38E},
  year         = {2009},
  journal = {Journal of Manual \& Manipulative Therapy},
}

@Misc{anova2,
  author = {Seng Chu},
  title  = {{Hypothesis Testing with ANOVA in Python}},
  note   = {Accessed on: 2019-12-09. [Online].},
  year   = {2019},
  owner  = {Coding disciple},
  url    = {https://codingdisciple.com/hypothesis-testing-ANOVA-python.html},
}

@Article{repeatability2,
  author       = {Bartlett, J. W. and Frost, C.},
  title        = {{Reliability, repeatability and reproducibility: analysis of measurement errors in continuous variables}},
  volume       = {31},
  number       = {4},
  pages        = {466--475},
  year         = {2008},
  journal = {Ultrasound in Obstetrics \& Gynecology},
}

@InProceedings{CAVE_paper,
  author    = {Biedenkapp, Andr{\'e} and Marben, Joshua and Lindauer, Marius and Hutter, Frank},
  title     = {{CAVE: Configuration Assessment, Visualization and Evaluation}},
  booktitle = {Learning and Intelligent Optimization},
  pages     = {115--130},
  publisher = {Springer International Publishing},
  abstract  = {To achieve peak performance of an algorithm (in particular for problems in AI), algorithm configuration is often necessary to determine a well-performing parameter configuration. So far, most studies in algorithm configuration focused on proposing better algorithm configuration procedures or on improving a particular algorithm's performance. In contrast, we use all the collected empirical performance data gathered during algorithm configuration runs to generate extensive insights into an algorithm, given problem instances and the used configurator. To this end, we provide a tool, called CAVE, that automatically generates comprehensive reports and insightful figures from all available empirical data. CAVE aims to help algorithm and configurator developers to better understand their experimental setup in an automated fashion. We showcase its use by thoroughly analyzing the well studied SAT solver spear on a benchmark of software verification instances and by empirically verifying two long-standing assumptions in algorithm configuration and parameter importance: (i) Parameter importance changes depending on the instance set at hand and (ii) Local and global parameter importance analysis do not necessarily agree with each other.},
  year      = {2019},
  location  = {Cham},
}

@Book{statistics_book,
  title     = {{Preliminary Edition of Statistics: Learning from Data}},
  publisher = {Cengage Learning},
  author    = {Peck, R.},
  year      = {2013},
}

@Article{scikit-learn,
  author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title        = {{Scikit-learn: Machine Learning in Python}},
  volume       = {12},
  pages        = {2825--2830},
  year         = {2011},
  journal = {Journal of Machine Learning Research},
}

@InProceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and Fabian Pedregosa and Andreas Mueller and Olivier Grisel and Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort and Jaques Grobler and Robert Layton and Jake VanderPlas and Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  pages     = {108--122},
  year      = {2013},
}

@Article{repeatability_final,
  author       = {Bland, J and Altman, Douglas},
  title        = {{Statistical-Methods For Assessing Agreement Between 2 Methods Of Clinical Measurement}},
  volume       = {47},
  year         = {2010},
  journal = {International journal of nursing studies},
}

@Misc{repeatability_final2,
  author = {Saskatchewan Highways and Transportation},
  title  = {{Statistical Quality Control Principles- Standard Test Procedures Manual}},
  year   = {1996},
  note   = {Accessed on: 2019-12-17. [Online].},
  owner  = {Saskatchewan Highways and Transportation},
  url    = {http://www.highways.gov.sk.ca/304-3/},
}

@Inproceedings{repeatability_final3,
  booktitle     = {{Standard Practice for Use of the Terms Precision and Bias in ASTM Test Methods}},
  publisher = {ASTM International},
  title    = {{Repeatability test procedure}},
  author = {ASTM International},
  year      = {2013},
}

@Article{SATZILLA,
  author       = {Xu, Lin and Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
  title        = {{SATzilla: Portfolio-based Algorithm Selection for SAT}},
  volume       = {32},
  pages        = {565--606},
  year         = {2008},
  journal = {Journal of Artificial Intelligence Research},
}

@InProceedings{t-SNE-original,
  author    = {Laurens van der Maaten and Geoffrey E. Hinton},
  title     = {{Visualizing Data using t-SNE}},
  booktitle = {Journal of Machine Learning Research},
  volume    = {9},
  pages     = {2579--2605},
  year      = {2008},
}

@Misc{GoogleML-course,
  author = {Google Developers},
  title  = {{Machine Learning Crash course}},
  note   = {Accessed on: 2019-12-17. [Online].},
  owner  = {Google},
  url    = {https://developers.google.com/machine-learning/crash-course},
}

@InProceedings{RF_categorical,
  author    = {T. {Bartz-Beielstein} and S. {Markon}},
  title     = {{Tuning search algorithms for real-world applications: a regression tree based approach}},
  booktitle = {Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)},
  volume    = {1},
  pages     = {1111-1118},
  abstract  = {The optimization of complex real-world problems might benefit from well tuned algorithm's parameters. We propose a methodology that performs this tuning in an effective and efficient algorithmical manner. This approach combines methods from statistical design of experiments, regression analysis, design and analysis of computer experiments methods, and tree-based regression. It can also be applied to analyze the influence of different operators or to compare the performance of different algorithms. An evolution strategy and a simulated annealing algorithm that optimize an elevator supervisory group controller system are used to demonstrate the applicability of our approach to real-world optimization problems.},
  year      = {2004},
  keywords  = {regression analysis;design of experiments;lifts;optimisation;evolutionary computation;simulated annealing;search problems;trees (mathematics);search algorithm tuning;optimization;statistical design;regression analysis;tree-based regression;evolution strategy;simulated annealing;elevator supervisory group controller system;Regression tree analysis;Application software;Design methodology;Regression analysis;Performance analysis;Algorithm design and analysis;Computational modeling;Simulated annealing;Elevators;Control system synthesis},
}

@InProceedings{RF_categorical2,
  author    = {Mustafa Baz and Brady Hunsaker},
  title     = {{Automated Tuning of Optimization Software Parameters}},
  booktitle = {Technical Report 2007-7},
  publisher = {University of Pittsburgh Department of Industrial Engineering Pittsburgh, PA},
  year      = {2007},
}

@Article{RF_mainpaper,
  author       = {Breiman, Leo},
  title        = {{Random Forests}},
  volume       = {45},
  number       = {1},
  pages        = {5--32},
  abstract     = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  year         = {2001},
  day          = {01},
  journal = {Machine Learning},
}

@Misc{smac-github,
  author       = {Marius Lindauer and Katharina Eggensperger and Matthias Feurer and Stefan Falkner and Andr{\'{e}} Biedenkapp and Frank Hutter},
  title        = {{SMAC v3: Algorithm Configuration in Python}},
  howpublished = {GitHub},
  year         = {2017},
  publisher    = {GitHub},
  url = {https://github.com/automl/SMAC3},
  note         = {Accessed on: 2019-05-22. [Online].},
}

@Incollection{EI_analytical,
  pages        = {117--129},
  title        = {{The application of Bayesian methods for seeking the extremum}},
  author       = {Mockus, J. and Tiesis, Vytautas and Zilinskas, Antanas},
  volume       = {2},
  year         = {2014},
  booktitle = {Towards Global Optimization},
}



@Article{EI_proper,
  author       = {{Brochu}, Eric and {Cora}, Vlad M. and {de Freitas}, Nando},
  title        = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
  pages        = {arXiv:1012.2599},
  adsnote      = {Provided by the SAO/NASA Astrophysics Data System},
  year         = {2010},
  eid          = {arXiv:1012.2599},
  eprint       = {1012.2599},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  journal = {arXiv e-prints},
  keywords     = {Computer Science - Machine Learning, G.1.6, G.3, I.2.6},
}

@InProceedings{RF_book,
author="Cutler, Adele
and Cutler, D. Richard
and Stevens, John R.",
title="Random Forests",
bookTitle="Ensemble Machine Learning: Methods and Applications",
year="2012",
publisher="Springer US",
journal = {Machine Learning},
pages="157--175",
abstract="Random Forests were introduced by Leo Breiman [6] who was inspired by earlier work by Amit and Geman [2]. Although not obvious from the description in [6], Random Forests are an extension of Breiman's bagging idea [5] and were developed as a competitor to boosting. Random Forests can be used for either a categorical response variable, referred to in [6] as ``classification,'' or a continuous response, referred to as ``regression.'' Similarly, the predictor variables can be either categorical or continuous.",
}

@Book{Bishop_ML_book,
  title     = {{Pattern Recognition and Machine Learning (Information Science and Statistics)}},
  publisher = {Springer-Verlag},
  author    = {Bishop, Christopher M.},
  year      = {2006},
  location  = {Berlin, Heidelberg},
}

@Article{SVM_tutorials,
  author       = {Burges, Christopher J. C.},
  title        = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
  volume       = {2},
  number       = {2},
  pages        = {121--167},
  acmid        = {593463},
  year         = {1998},
  issue_date   = {June 1998},
  journal = {Data Min. Knowl. Discov.},
  keywords     = {VC dimension, pattern recognition, statistical learning theory, support vector machines},
  location     = {Norwell, MA, USA},
  numpages     = {47},
  publisher    = {Kluwer Academic Publishers},
}

@InProceedings{KNN,
  author    = {Guo, Gongde and Wang, Hui and Bell, David and Bi, Yaxin and Greer, Kieran},
  title     = {{KNN Model-Based Approach in Classification}},
  booktitle = {On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE},
  pages     = {986--996},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency -- being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a ``good value'' for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN.},
  year      = {2003},
  location  = {Berlin, Heidelberg},
}

@Article{GBM,
  author       = {Friedman, Jerome},
  title        = {{Greedy Function Approximation: A Gradient Boosting Machine}},
  volume       = {29},
  year         = {2000},
  journal = {The Annals of Statistics},
}

@Article{QRF,
  author       = {Meinshausen, Nicolai},
  title        = {{Quantile Regression Forests}},
  volume       = {7},
  pages        = {983--999},
  acmid        = {1248582},
  year         = {2006},
  issue_date   = {12/1/2006},
  journal = {Journal of Machine Learning Research},
  numpages     = {17},
  publisher    = {JMLR.org},
}

@InCollection{FSI_properties,
  author    = {Qun Zhang and Song Cen},	
  title     = {{13 - Multiphysics modeling for biomechanical problems}},
  booktitle = {{Multiphysics Modeling}},
  publisher = {Academic Press},
  series    = {Elsevier and Tsinghua University Press Computational Mechanics Series},
  pages     = {363--373},
  abstract  = {In the biomechanical field, fluidâ€“structure interaction (FSI) analysis is conducted to simulate blood flowblood flow in the deformable vessel wall and the real heart. In this chapter, the first case presented is a transient FSIFSI analysis of a 3D simplified artificial heartartificial heart. The second case is fluid-structure coupling simulation of a vascular tumorvascular tumor. The strong coupling methodstrong coupling method is used in both the cases.},
  year      = {2016},
  keywords  = {FSI, blood flow, strong coupling method, artificial heart, vascular tumor, ALE, buckling, ADINA, INTESIM, MITC shell},
  location  = {Oxford},
}

@InCollection{FSI_properties2,
  author    = {O.C. Zienkiewicz and R.L. Taylor and P. Nithiarasu},
  title     = {{Fluid--Structure Interaction}},
  booktitle = {The Finite Element Method for Fluid Dynamics},
  publisher = {Butterworth-Heinemann},
  pages     = {423--449},
  edition   = {Seventh},
  abstract  = {This chapter provides a brief introduction to the vast area of fluidâ€“structure interaction. The topics covered include flexible tubes and multidimensional problems of fluid-structure interaction. Both the monolithic and segregated approaches are discussed along with some important mesh moving strategies.},
  year      = {2014},
  keywords  = {one-dimensional, flexible tubes, multidimensional problems, monolithic and segregated approaches, strong coupling, mesh moving, spring analogy, partial differential equations for mesh moving},
  location  = {Oxford},
}

@PhdThesis{Hutterphd,
  author      = {Frank Hutter},
  title       = {{Automated Configuration of Algorithms for Solving Hard Computational Problems}},
  year        = {2009},
  school = {University of British Columbia, Department of Computer Science},
  lauthor     = {Frank Hutter},
  location    = {Vancouver, Canada},
}

@Article{confusionmatrix-CV,
  author       = {Forman, George and Scholz, Martin},
  title        = {{Apples-to-apples in Cross-validation Studies: Pitfalls in Classifier Performance Measurement}},
  volume       = {12},
  number       = {1},
  pages        = {49--57},
  acmid        = {1882479},
  year         = {2010},
  issue_date   = {June 2010},
  journal = {SIGKDD Explor. Newsl.},
  location     = {New York, NY, USA},
  numpages     = {9},
  publisher    = {ACM},
}

@InProceedings{RF_for_IF,
  author    = {C\'{a}ceres, Leslie P{\'e}rez and Bischl, Bernd and St\"{u}tzle, Thomas},
  title     = {{Evaluating Random Forest Models for Irace}},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
  series    = {GECCO '17},
  pages     = {1146--1153},
  acmid     = {3082057},
  year      = {2017},
  keywords  = {automatic algorithm configuration, irace, parameter tuning, random forests},
  location  = {Berlin, Germany},
  numpages  = {8},
}

@InCollection{Localsearch_constraintbook,
  author    = {Rina Dechter},
  title     = {{Stochastic Greedy Local Search}},
  booktitle = {Constraint Processing},
  publisher = {Morgan Kaufmann},
  series    = {The Morgan Kaufmann Series in Artificial Intelligence},
  pages     = {191--208},
  year      = {2003},
  location  = {San Francisco},
}

@Book{Russell_Norvig,
  title     = {{Artificial Intelligence: A Modern Approach}},
  publisher = {Prentice Hall Press},
  author    = {Russell, Stuart and Norvig, Peter},
  edition   = {3\textsuperscript{rd}},
  year      = {2009},
  location  = {Upper Saddle River, NJ, USA},
}

@InCollection{GA_disadv1,
  author    = {Xin-She Yang},
  title     = {{Genetic Algorithms}},
  booktitle = {Nature-Inspired Optimization Algorithms},
  publisher = {Elsevier},
  pages     = {77--87},
  abstract  = {Genetic algorithms are among the most popular evolutionary algorithms in terms of the diversity of their applications. A vast majority of well-known optimization problems have been solved using genetic algorithms. In addition, genetic algorithms are population-based, and many modern evolutionary algorithms are directly based on genetic algorithms or have some strong similarities to them.},
  year      = {2014},
  keywords  = {Genetic algorithms, Genetic operators, Evolutionary algorithm, Optimization},
  location  = {Oxford},
}

@InCollection{GA_disadv2,
  author    = {Anke Meyer-Baese and Volker Schmid},
  title     = {{Genetic Algorithms}},
  booktitle = {Pattern Recognition and Signal Analysis in Medical Imaging (Second Edition)},
  publisher = {Academic Press},
  pages     = {135--149},
  edition   = {Second},
  abstract  = {Genetic algorithms (GA) like neural networks are biologically inspired and represent a new computational model having its roots in evolutionary sciences. In this chapter, we review the basics of GAs, briefly describe the schema theorem and the building block hypothesis, and describe feature selection based on GAs, as one of the most important applications of GAs.},
  year      = {2014},
  keywords  = {Genetic algorithms, Optimization, Canonical genetic algorithm, Genetic operators, Schema theorem, Building block hypothesis},
  location  = {Oxford},
}

@Article{aerodynamics_time,
  author       = {Dr. Marc Ratzel, Thomas Ludescher},
  title        = {{Streamlining Aerodynamic CFD Analyses}},
  year         = {2013},
  journal = {NAFEMS World Congress},
}

@InProceedings{mesh_size1,
  author       = {Liu, Yucheng},
  title        = {{Effects of Mesh Density on Finite Element Analysis}},
  volume       = {2},
  year         = {2013},
  journal = {SAE Technical Papers},
}

@Article{mesh_size2,
  author       = {Kurra Suresh and Srinivasa Prakash Regalla},
  title        = {{Effect of Mesh Parameters in Finite Element Simulation of Single Point Incremental Sheet Forming Process}},
  volume       = {6},
  pages        = {376--382},
  note         = {3\textsuperscript{rd} International Conference on Materials Processing and Characterisation (ICMPC 2014)},
  abstract     = {This paper investigates the effect of element size and adaptive re-meshing technique in numerical simulation of incremental sheet forming (ISF) process. In ISF a hemispherical headed tool moves along the specified trajectory to deform the sheet in to required shape. This tool path is generally very long and thus increases the computational time. Therefore, in this work adaptive re- meshing technique has been used to minimize the computational time without sacrificing the accuracy of the results. For this a varying wall angle conical frustum was simulated using shell elements with different element edge lengths and adaptive mesh. Effects of these mesh parameters on plastic strain, punch force and form accuracy of deformed geometry has been studied. The necessary simulations for this study are performed using explicit finite element code LS-DYNA.},
  year         = {2014},
  journal = {Procedia Materials Science},
  keywords     = {Incremental forming, Adaptive mesh, Numerical simulation, Plastic strain, Punch force.},
}

@Misc{aeroplane_image,
  author = {Patrick Hanley},
  title  = {{Hanley innovations upgrades stallion 3D to Version 5.0}},
  note   = {Accessed on: 2019-01-03. [Online].},
  owner  = {DIY DRONES},
  url    = {https://diydrones.com/profiles/blogs/hanley-innovations-upgrades-stallion-3d-to-version-5-0},
}

@Misc{Robots_applications,
  author = {Multi-Physics in Robotics Lab},
  title  = {{Robots}},
  note   = {Accessed on: 2019-01-03. [Online].},
  owner  = {Multi-Physics in Robotics Lab},
  url    = {https://mpirl.com/portfolio/robotics-2/},
}

@Misc{martin_bo_tutorial,
  author       = {Martin Krasser},
  title        = {{Bayesian Optimization}},
  howpublished = {GitHub},
  note         = {Accessed on: 2019-11-12. [Online].},
  year         = {2018},
  url          = {https://github.com/krasserm/bayesian-machine-learning},
}

@Comment{jabref-meta: databaseType:bibtex;}